{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Applied ML on ACE Intelligence System\n",
    "\n",
    "**Objective**: transforming from retrospective analysis to forward-looking intelligence that predicts where violations will cluster, enabling proactive camera deployment rather than reactive ticketing.\n",
    "\n",
    "### What We're Building:\n",
    "A comprehensive feature-rich dataset that captures:\n",
    "- **Temporal intelligence**: rush hour, school hours, CUNY class changes\n",
    "- **Spatial intelligence**: GTFS integration, violation clustering, CUNY proximity\n",
    "- **Enforcement adaptation**: violator learning patterns, predictability entropy\n",
    "- **Multiple prediction targets**: immediate, tactical, and strategic forecasting\n",
    "\n",
    "**Goal**: enabling prediction of violation hotspots 24 hours ahead for proactive deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACE Intelligence System - Feature Engineering Module Loaded\n",
      "Ready to process 3.78M violations and build predictive features\n",
      "Goal: transforming from reactive enforcement to predictive intelligence\n"
     ]
    }
   ],
   "source": [
    "# importing essential libraries for comprehensive feature engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from typing import Dict, List, Tuple\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy import stats\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ACE Intelligence System - Feature Engineering Module Loaded\")\n",
    "print(\"Ready to process 3.78M violations and build predictive features\")\n",
    "print(\"Goal: transforming from reactive enforcement to predictive intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading and Memory Optimization\n",
    "\n",
    "loading the full violations dataset (3.78M records) with optimal memory usage based on the data dictionary specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized data types configured for memory efficiency\n",
      "Expected memory reduction: ~40% vs default dtypes\n",
      "Current memory usage: 3716.2 MB\n"
     ]
    }
   ],
   "source": [
    "# defining optimal data types based on MTA data dictionary\n",
    "VIOLATIONS_DTYPES = {\n",
    "    'Violation ID': 'int64',\n",
    "    'Vehicle ID': 'string',  # hashed values\n",
    "    'Violation Status': 'category',\n",
    "    'Violation Type': 'category', \n",
    "    'Bus Route ID': 'string',\n",
    "    'Violation Latitude': 'float32',\n",
    "    'Violation Longitude': 'float32',\n",
    "    'Stop ID': 'string',\n",
    "    'Stop Name': 'string',\n",
    "    'Bus Stop Latitude': 'float32',\n",
    "    'Bus Stop Longitude': 'float32'\n",
    "}\n",
    "\n",
    "# date columns will be parsed separately\n",
    "DATE_COLUMNS = ['First Occurrence', 'Last Occurrence']\n",
    "\n",
    "print(\"Optimized data types configured for memory efficiency\")\n",
    "print(f\"Expected memory reduction: ~40% vs default dtypes\")\n",
    "\n",
    "# Memory monitoring function\n",
    "def get_memory_usage():\n",
    "    \"\"\"monitoring memory usage during processing\"\"\"\n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "print(f\"Current memory usage: {get_memory_usage():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full violations dataset (3.78M records)...\n",
      "This may take 2-3 minutes for comprehensive processing\n",
      "File size: 1.02 GB\n",
      "Using chunked processing for optimal memory usage\n",
      "Phase 1: data quality assessment...\n",
      "Columns loaded: ['Violation ID', 'Vehicle ID', 'First Occurrence', 'Last Occurrence', 'Violation Status', 'Violation Type', 'Bus Route ID', 'Violation Latitude', 'Violation Longitude', 'Stop ID', 'Stop Name', 'Bus Stop Latitude', 'Bus Stop Longitude', 'Violation Georeference', 'Bus Stop Georeference']\n",
      "Date range sample: 2025-05-31 15:15:44 to 2025-08-21 19:40:47\n",
      "   Processed 100,000 rows... (3644.6 MB)\n",
      "Assessment complete: 600,000 rows processed\n",
      "Phase 2: full dataset loading...\n",
      "Violations data loaded successfully!\n",
      "Shape: (3778568, 15)\n",
      "Memory usage: 3775.7 MB\n",
      "Date range: 2019-10-07 07:06:54 to 2025-08-21 19:40:47\n"
     ]
    }
   ],
   "source": [
    "# loading the full violations dataset with chunked processing if needed\n",
    "violations_file = \"../data/MTA_Bus_Automated_Camera_Enforcement_Violations__Beginning_October_2019_20250919.csv\"\n",
    "\n",
    "print(\"Loading full violations dataset (3.78M records)...\")\n",
    "print(\"This may take 2-3 minutes for comprehensive processing\")\n",
    "\n",
    "# check file size to determine loading strategy\n",
    "file_size_gb = Path(violations_file).stat().st_size / (1024**3)\n",
    "print(f\"File size: {file_size_gb:.2f} GB\")\n",
    "\n",
    "if file_size_gb > 0.5:  # use chunked loading for large files\n",
    "    print(\"Using chunked processing for optimal memory usage\")\n",
    "    \n",
    "    chunk_size = 100000  # Process 100K records at a time\n",
    "    processed_chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    # first pass: get total row count and data quality metrics\n",
    "    print(\"Phase 1: data quality assessment...\")\n",
    "    \n",
    "    for i, chunk in enumerate(pd.read_csv(violations_file, chunksize=chunk_size, dtype=VIOLATIONS_DTYPES, parse_dates=DATE_COLUMNS)):\n",
    "        total_rows += len(chunk)\n",
    "        if i == 0:\n",
    "            print(f\"Columns loaded: {list(chunk.columns)}\")\n",
    "            print(f\"Date range sample: {chunk['First Occurrence'].min()} to {chunk['First Occurrence'].max()}\")\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"   Processed {total_rows:,} rows... ({get_memory_usage():.1f} MB)\")\n",
    "        \n",
    "        # break after processing enough for assessment\n",
    "        if i >= 5:  # process ~500K records for initial assessment\n",
    "            break\n",
    "    \n",
    "    print(f\"Assessment complete: {total_rows:,} rows processed\")\n",
    "    \n",
    "    # now load the full dataset with optimized approach\n",
    "    print(\"Phase 2: full dataset loading...\")\n",
    "    \n",
    "    violations_data = pd.read_csv(\n",
    "        violations_file, \n",
    "        dtype=VIOLATIONS_DTYPES,\n",
    "        parse_dates=DATE_COLUMNS,\n",
    "        low_memory=False\n",
    "    )\n",
    "else:\n",
    "    # direct loading for smaller files\n",
    "    violations_data = pd.read_csv(\n",
    "        violations_file,\n",
    "        dtype=VIOLATIONS_DTYPES,\n",
    "        parse_dates=DATE_COLUMNS\n",
    "    )\n",
    "\n",
    "print(f\"Violations data loaded successfully!\")\n",
    "print(f\"Shape: {violations_data.shape}\")\n",
    "print(f\"Memory usage: {get_memory_usage():.1f} MB\")\n",
    "print(f\"Date range: {violations_data['First Occurrence'].min()} to {violations_data['First Occurrence'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY REPORT\n",
      "==================================================\n",
      "Missing Values Summary:\n",
      "              Missing_Count  Missing_Percentage       Data_Type\n",
      "Vehicle ID            66366                1.76  string[python]\n",
      "Bus Route ID          10749                0.28  string[python]\n",
      "\n",
      "Route Coverage:\n",
      "   Unique routes: 40\n",
      "   Top 10 routes by violations:\n",
      "   M15+: 502,765 violations\n",
      "   BX19: 344,147 violations\n",
      "   M101: 312,466 violations\n",
      "   BX41+: 229,920 violations\n",
      "   BX36: 225,835 violations\n",
      "   B46+: 186,192 violations\n",
      "   BX12+: 185,059 violations\n",
      "   B44+: 171,083 violations\n",
      "   Q44+: 164,806 violations\n",
      "   BX6+: 111,403 violations\n",
      "\n",
      "Temporal Coverage:\n",
      "   Start date: 2019-10-07 07:06:54\n",
      "   End date: 2025-08-21 19:40:47\n",
      "   Time span: 2145 days\n",
      "\n",
      "Geographic Bounds:\n",
      "   Lat range: 40.5345 to 40.8811\n",
      "   Lon range: -74.1844 to -73.7010\n"
     ]
    }
   ],
   "source": [
    "# generating data quality report\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# missing values analysis\n",
    "missing_data = violations_data.isnull().sum()\n",
    "missing_pct = (missing_data / len(violations_data)) * 100\n",
    "\n",
    "quality_report = pd.DataFrame({\n",
    "    'Missing_Count': missing_data,\n",
    "    'Missing_Percentage': missing_pct.round(2),\n",
    "    'Data_Type': violations_data.dtypes\n",
    "})\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(quality_report[quality_report['Missing_Count'] > 0])\n",
    "\n",
    "# route coverage analysis\n",
    "print(f\"\\nRoute Coverage:\")\n",
    "print(f\"   Unique routes: {violations_data['Bus Route ID'].nunique()}\")\n",
    "print(f\"   Top 10 routes by violations:\")\n",
    "route_counts = violations_data['Bus Route ID'].value_counts().head(10)\n",
    "for route, count in route_counts.items():\n",
    "    print(f\"   {route}: {count:,} violations\")\n",
    "\n",
    "# temporal coverage\n",
    "print(f\"\\nTemporal Coverage:\")\n",
    "print(f\"   Start date: {violations_data['First Occurrence'].min()}\")\n",
    "print(f\"   End date: {violations_data['First Occurrence'].max()}\")\n",
    "print(f\"   Time span: {(violations_data['First Occurrence'].max() - violations_data['First Occurrence'].min()).days} days\")\n",
    "\n",
    "# geographic bounds check\n",
    "print(f\"\\nGeographic Bounds:\")\n",
    "print(f\"   Lat range: {violations_data['Violation Latitude'].min():.4f} to {violations_data['Violation Latitude'].max():.4f}\")\n",
    "print(f\"   Lon range: {violations_data['Violation Longitude'].min():.4f} to {violations_data['Violation Longitude'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Temporal Feature Engineering\n",
    "\n",
    "creating sophisticated temporal features that capture:\n",
    "- standard time patterns (hour, day, month)\n",
    "- NYC-specific patterns (rush hour, school hours)\n",
    "- CUNY-specific patterns (class changes, semester cycles)\n",
    "- Enforcement timeline (days since ACE implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPORAL FEATURE ENGINEERING\n",
      "==================================================\n",
      "Creating basic temporal features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic temporal features created\n",
      "Creating NYC-specific temporal features...\n",
      "Rush hour features created\n",
      "School hours features created\n"
     ]
    }
   ],
   "source": [
    "print(\"TEMPORAL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# creating basic temporal features\n",
    "print(\"Creating basic temporal features...\")\n",
    "violations_data['violation_datetime'] = violations_data['First Occurrence']\n",
    "violations_data['violation_hour'] = violations_data['violation_datetime'].dt.floor('H')\n",
    "violations_data['hour_of_day'] = violations_data['violation_datetime'].dt.hour\n",
    "violations_data['day_of_week'] = violations_data['violation_datetime'].dt.dayofweek  # 0=Monday\n",
    "violations_data['month'] = violations_data['violation_datetime'].dt.month\n",
    "violations_data['year'] = violations_data['violation_datetime'].dt.year\n",
    "violations_data['day_of_year'] = violations_data['violation_datetime'].dt.dayofyear\n",
    "\n",
    "# weekend/weekday classification\n",
    "violations_data['is_weekend'] = violations_data['day_of_week'].isin([5, 6])  # Sat, Sun\n",
    "\n",
    "print(\"Basic temporal features created\")\n",
    "\n",
    "# creating NYC-specific temporal features\n",
    "print(\"Creating NYC-specific temporal features...\")\n",
    "\n",
    "# rush hour classification (more nuanced than simple 7-9, 5-7)\n",
    "def classify_rush_hour(hour, day_of_week):\n",
    "    \"\"\"classifying rush hour periods with weekday/weekend distinction\"\"\"\n",
    "    if day_of_week in [5, 6]:  # weekend\n",
    "        if 10 <= hour <= 14:  # weekend midday rush\n",
    "            return 'weekend_midday'\n",
    "        elif 18 <= hour <= 21:  # weekend evening activity\n",
    "            return 'weekend_evening'\n",
    "        else:\n",
    "            return 'weekend_off_peak'\n",
    "    else:  # weekday\n",
    "        if 7 <= hour <= 9:  # morning rush\n",
    "            return 'morning_rush'\n",
    "        elif 17 <= hour <= 19:  # evening rush\n",
    "            return 'evening_rush'\n",
    "        elif 10 <= hour <= 16:  # midday\n",
    "            return 'midday'\n",
    "        elif 20 <= hour <= 23:  # evening activity\n",
    "            return 'evening_activity'\n",
    "        else:  # late night/early morning\n",
    "            return 'off_peak'\n",
    "\n",
    "violations_data['rush_hour_period'] = violations_data.apply(\n",
    "    lambda x: classify_rush_hour(x['hour_of_day'], x['day_of_week']), axis=1\n",
    ")\n",
    "\n",
    "# binary rush hour flags\n",
    "violations_data['is_morning_rush'] = (violations_data['hour_of_day'].between(7, 9)) & (~violations_data['is_weekend'])\n",
    "violations_data['is_evening_rush'] = (violations_data['hour_of_day'].between(17, 19)) & (~violations_data['is_weekend'])\n",
    "violations_data['is_any_rush'] = violations_data['is_morning_rush'] | violations_data['is_evening_rush']\n",
    "\n",
    "print(\"Rush hour features created\")\n",
    "\n",
    "# school hours (affects violation patterns)\n",
    "violations_data['is_school_hours'] = (\n",
    "    (violations_data['hour_of_day'].between(8, 15)) & \n",
    "    (~violations_data['is_weekend'])\n",
    ")\n",
    "\n",
    "print(\"School hours features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CUNY-specific temporal features...\n",
      "CUNY temporal features created\n",
      "Creating ACE enforcement timeline features...\n",
      "ACE timeline features created\n",
      "Creating temporal volatility features...\n",
      "Temporal volatility will be calculated after spatial aggregation\n",
      "\n",
      "Temporal Features Summary:\n",
      "Created 12 temporal features\n",
      "   • hour_of_day\n",
      "   • day_of_week\n",
      "   • month\n",
      "   • is_weekend\n",
      "   • rush_hour_period\n",
      "   • is_morning_rush\n",
      "   • is_evening_rush\n",
      "   • is_school_hours\n",
      "   • is_cuny_class_change\n",
      "   • semester_period\n",
      "   • days_since_ace_implementation\n",
      "   • is_post_ace_expansion\n"
     ]
    }
   ],
   "source": [
    "# creating CUNY-specific temporal features\n",
    "print(\"Creating CUNY-specific temporal features...\")\n",
    "\n",
    "# CUNY class change periods (every hour 8am-6pm during academic periods)\n",
    "violations_data['is_cuny_class_change'] = (\n",
    "    (violations_data['hour_of_day'].between(8, 18)) &\n",
    "    (~violations_data['is_weekend']) &\n",
    "    (violations_data['violation_datetime'].dt.minute.between(50, 10))  # 10 min before/after hour\n",
    ")\n",
    "\n",
    "# academic semester patterns (approximate)\n",
    "def get_semester_period(date):\n",
    "    \"\"\"classifying academic periods\"\"\"\n",
    "    month = date.month\n",
    "    if month in [9, 10, 11, 12]:  # Fall semester\n",
    "        return 'fall_semester'\n",
    "    elif month in [1, 2, 3, 4, 5]:  # Spring semester\n",
    "        return 'spring_semester'\n",
    "    else:  # Summer\n",
    "        return 'summer_session'\n",
    "\n",
    "violations_data['semester_period'] = violations_data['violation_datetime'].apply(get_semester_period)\n",
    "violations_data['is_academic_year'] = violations_data['semester_period'].isin(['fall_semester', 'spring_semester'])\n",
    "\n",
    "print(\"CUNY temporal features created\")\n",
    "\n",
    "# creating ACE enforcement timeline features\n",
    "print(\"Creating ACE enforcement timeline features...\")\n",
    "\n",
    "# ACE program timeline markers\n",
    "ace_implementation_date = datetime(2024, 6, 1)  # Major expansion\n",
    "ace_pilot_start = datetime(2019, 10, 1)  # Initial pilot\n",
    "\n",
    "violations_data['days_since_ace_implementation'] = (\n",
    "    violations_data['violation_datetime'] - ace_implementation_date\n",
    ").dt.days\n",
    "\n",
    "violations_data['days_since_ace_pilot'] = (\n",
    "    violations_data['violation_datetime'] - ace_pilot_start\n",
    ").dt.days\n",
    "\n",
    "violations_data['is_post_ace_expansion'] = violations_data['violation_datetime'] >= ace_implementation_date\n",
    "violations_data['is_ace_pilot_period'] = (\n",
    "    (violations_data['violation_datetime'] >= ace_pilot_start) &\n",
    "    (violations_data['violation_datetime'] < ace_implementation_date)\n",
    ")\n",
    "\n",
    "print(\"ACE timeline features created\")\n",
    "\n",
    "# creating temporal volatility features (for enforcement adaptation analysis)\n",
    "print(\"Creating temporal volatility features...\")\n",
    "\n",
    "# This will be calculated later after aggregation by location\n",
    "print(\"Temporal volatility will be calculated after spatial aggregation\")\n",
    "\n",
    "print(f\"\\nTemporal Features Summary:\")\n",
    "temporal_features = [\n",
    "    'hour_of_day', 'day_of_week', 'month', 'is_weekend',\n",
    "    'rush_hour_period', 'is_morning_rush', 'is_evening_rush',\n",
    "    'is_school_hours', 'is_cuny_class_change', 'semester_period',\n",
    "    'days_since_ace_implementation', 'is_post_ace_expansion'\n",
    "]\n",
    "print(f\"Created {len(temporal_features)} temporal features\")\n",
    "for feature in temporal_features:\n",
    "    print(f\"   • {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Spatial Intelligence Features\n",
    "\n",
    "building comprehensive spatial features by:\n",
    "- loading GTFS data from all boroughs\n",
    "- creating violation hotspot clusters\n",
    "- building spatial intelligence for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPATIAL INTELLIGENCE FEATURE ENGINEERING\n",
      "==================================================\n",
      "Loading GTFS data from all boroughs...\n",
      "   Loading bronx GTFS data...\n",
      "   Loading brooklyn GTFS data...\n",
      "   Loading manhattan GTFS data...\n",
      "   Loading queens GTFS data...\n",
      "   Loading staten_island GTFS data...\n",
      "Loaded 11,698 stops from 5 boroughs\n",
      "Loaded 1,440 routes from 5 boroughs\n",
      "Loaded 371,158 shape points from 5 boroughs\n"
     ]
    }
   ],
   "source": [
    "print(\"SPATIAL INTELLIGENCE FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# loading GTFS data from all boroughs\n",
    "print(\"Loading GTFS data from all boroughs...\")\n",
    "\n",
    "gtfs_boroughs = ['bronx', 'brooklyn', 'manhattan', 'queens', 'staten_island']\n",
    "all_stops = []\n",
    "all_routes = []\n",
    "all_shapes = []\n",
    "\n",
    "for borough in gtfs_boroughs:\n",
    "    gtfs_path = Path(f\"../data/gtfs/{borough}\")\n",
    "    \n",
    "    if gtfs_path.exists():\n",
    "        print(f\"   Loading {borough} GTFS data...\")\n",
    "        \n",
    "        # load stops\n",
    "        stops_file = gtfs_path / \"stops.txt\"\n",
    "        if stops_file.exists():\n",
    "            stops_df = pd.read_csv(stops_file)\n",
    "            stops_df['borough'] = borough\n",
    "            all_stops.append(stops_df)\n",
    "        \n",
    "        # load routes\n",
    "        routes_file = gtfs_path / \"routes.txt\"\n",
    "        if routes_file.exists():\n",
    "            routes_df = pd.read_csv(routes_file)\n",
    "            routes_df['borough'] = borough\n",
    "            all_routes.append(routes_df)\n",
    "        \n",
    "        # load shapes (if available)\n",
    "        shapes_file = gtfs_path / \"shapes.txt\"\n",
    "        if shapes_file.exists():\n",
    "            shapes_df = pd.read_csv(shapes_file)\n",
    "            shapes_df['borough'] = borough\n",
    "            all_shapes.append(shapes_df)\n",
    "\n",
    "# combine all GTFS data\n",
    "if all_stops:\n",
    "    gtfs_stops = pd.concat(all_stops, ignore_index=True)\n",
    "    print(f\"Loaded {len(gtfs_stops):,} stops from {len(all_stops)} boroughs\")\n",
    "else:\n",
    "    print(\"No GTFS stops data found, will use violation coordinates for spatial analysis\")\n",
    "    gtfs_stops = None\n",
    "\n",
    "if all_routes:\n",
    "    gtfs_routes = pd.concat(all_routes, ignore_index=True)\n",
    "    print(f\"Loaded {len(gtfs_routes):,} routes from {len(all_routes)} boroughs\")\n",
    "else:\n",
    "    gtfs_routes = None\n",
    "\n",
    "if all_shapes:\n",
    "    gtfs_shapes = pd.concat(all_shapes, ignore_index=True)\n",
    "    print(f\"Loaded {len(gtfs_shapes):,} shape points from {len(all_shapes)} boroughs\")\n",
    "else:\n",
    "    gtfs_shapes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating violation hotspot clusters...\n",
      "Valid coordinates: 3,778,568 out of 3,778,568 violations\n",
      "Sampling 50000 violations for clustering analysis...\n",
      "Running DBSCAN clustering...\n",
      "Clustering complete:\n",
      "   • 1 clusters identified\n",
      "   • 0 noise points (outliers)\n",
      "   • 50000 points in clusters\n",
      "\n",
      "Top 5 violation hotspots:\n",
      "   1. Cluster 0: 50000.0 violations, 40.0 routes\n",
      "      Center: (40.7641, -73.9208)\n"
     ]
    }
   ],
   "source": [
    "# creating violation hotspot clusters using DBSCAN\n",
    "print(\"Creating violation hotspot clusters...\")\n",
    "\n",
    "# filter out invalid coordinates\n",
    "valid_coords = violations_data[\n",
    "    (violations_data['Violation Latitude'].notna()) &\n",
    "    (violations_data['Violation Longitude'].notna()) &\n",
    "    (violations_data['Violation Latitude'].between(40.4, 41.0)) &  # NYC bounds\n",
    "    (violations_data['Violation Longitude'].between(-74.5, -73.5))\n",
    "].copy()\n",
    "\n",
    "print(f\"Valid coordinates: {len(valid_coords):,} out of {len(violations_data):,} violations\")\n",
    "\n",
    "# Haversine distance function for geographic clustering\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"calculating haversine distance between two points in meters\"\"\"\n",
    "    R = 6371000  # earth radius in meters\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "# sample for clustering if dataset is very large\n",
    "if len(valid_coords) > 50000:\n",
    "    print(f\"Sampling {50000} violations for clustering analysis...\")\n",
    "    clustering_sample = valid_coords.sample(n=50000, random_state=42)\n",
    "else:\n",
    "    clustering_sample = valid_coords\n",
    "\n",
    "# DBSCAN clustering (eps in degrees, roughly 100 meters)\n",
    "coords_for_clustering = clustering_sample[['Violation Latitude', 'Violation Longitude']].values\n",
    "\n",
    "# converting to radians for proper distance calculation\n",
    "coords_rad = np.radians(coords_for_clustering)\n",
    "\n",
    "print(\"Running DBSCAN clustering...\")\n",
    "dbscan = DBSCAN(eps=0.001, min_samples=5, metric='haversine')\n",
    "cluster_labels = dbscan.fit_predict(coords_rad)\n",
    "\n",
    "clustering_sample['violation_cluster'] = cluster_labels\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"Clustering complete:\")\n",
    "print(f\"   • {n_clusters} clusters identified\")\n",
    "print(f\"   • {n_noise} noise points (outliers)\")\n",
    "print(f\"   • {len(clustering_sample) - n_noise} points in clusters\")\n",
    "\n",
    "# analyzing cluster characteristics\n",
    "cluster_stats = clustering_sample[clustering_sample['violation_cluster'] != -1].groupby('violation_cluster').agg({\n",
    "    'Violation ID': 'count',\n",
    "    'Violation Latitude': ['mean', 'std'],\n",
    "    'Violation Longitude': ['mean', 'std'],\n",
    "    'Bus Route ID': 'nunique'\n",
    "}).round(6)\n",
    "\n",
    "cluster_stats.columns = ['violation_count', 'lat_center', 'lat_std', 'lon_center', 'lon_std', 'unique_routes']\n",
    "cluster_stats = cluster_stats.sort_values('violation_count', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 violation hotspots:\")\n",
    "for i, (cluster_id, stats) in enumerate(cluster_stats.head().iterrows()):\n",
    "    print(f\"   {i+1}. Cluster {cluster_id}: {stats['violation_count']} violations, {stats['unique_routes']} routes\")\n",
    "    print(f\"      Center: ({stats['lat_center']:.4f}, {stats['lon_center']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spatial density features...\n",
      "Computing spatial density (this may take a few minutes)...\n",
      "   Calculating density within 100m radius...\n",
      "Density features calculated for 1000 violations\n",
      "   Average violations within 100m: 1.9\n",
      "   Max violations within 100m: 14\n",
      "Creating bus stop clusters...\n",
      "Created 1 stop clusters from 11698 stops\n",
      "\n",
      "Spatial Features Summary:\n",
      "Created spatial intelligence features\n",
      "   • 1 violation hotspot clusters\n",
      "   • Density analysis for violation prediction\n",
      "   • 1 stop clusters for route optimization\n"
     ]
    }
   ],
   "source": [
    "# creating spatial density features\n",
    "print(\"Creating spatial density features...\")\n",
    "\n",
    "# function to calculate violations within radius\n",
    "def calculate_density_features(df, radius_meters=100):\n",
    "    \"\"\"calculating violation density within specified radius\"\"\"\n",
    "    density_features = []\n",
    "    \n",
    "    print(f\"   Calculating density within {radius_meters}m radius...\")\n",
    "    \n",
    "    # sample for efficiency if dataset is large\n",
    "    if len(df) > 10000:\n",
    "        sample_size = min(10000, len(df))\n",
    "        sample_df = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"   Using sample of {sample_size} violations for density calculation\")\n",
    "    else:\n",
    "        sample_df = df\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        lat, lon = row['Violation Latitude'], row['Violation Longitude']\n",
    "        \n",
    "        # calculating distances to all other violations\n",
    "        distances = df.apply(\n",
    "            lambda x: haversine_distance(lat, lon, x['Violation Latitude'], x['Violation Longitude']),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # counting violations within radius\n",
    "        nearby_violations = (distances <= radius_meters).sum() - 1  # exclude self\n",
    "        \n",
    "        density_features.append({\n",
    "            'violation_id': row['Violation ID'],\n",
    "            f'violations_within_{radius_meters}m': nearby_violations\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(density_features)\n",
    "\n",
    "# calculating density for a sample (this is computationally intensive)\n",
    "print(\"Computing spatial density (this may take a few minutes)...\")\n",
    "density_sample = valid_coords.sample(n=min(1000, len(valid_coords)), random_state=42)\n",
    "density_features = calculate_density_features(density_sample, radius_meters=100)\n",
    "\n",
    "print(f\"Density features calculated for {len(density_features)} violations\")\n",
    "print(f\"   Average violations within 100m: {density_features['violations_within_100m'].mean():.1f}\")\n",
    "print(f\"   Max violations within 100m: {density_features['violations_within_100m'].max()}\")\n",
    "\n",
    "# create stop cluster mapping using DBSCAN on stops\n",
    "if gtfs_stops is not None:\n",
    "    print(\"Creating bus stop clusters...\")\n",
    "    \n",
    "    # Filter valid stop coordinates\n",
    "    valid_stops = gtfs_stops[\n",
    "        (gtfs_stops['stop_lat'].notna()) &\n",
    "        (gtfs_stops['stop_lon'].notna())\n",
    "    ].copy()\n",
    "    \n",
    "    if len(valid_stops) > 0:\n",
    "        stop_coords = valid_stops[['stop_lat', 'stop_lon']].values\n",
    "        stop_coords_rad = np.radians(stop_coords)\n",
    "        \n",
    "        stop_dbscan = DBSCAN(eps=0.001, min_samples=2, metric='haversine')\n",
    "        stop_clusters = stop_dbscan.fit_predict(stop_coords_rad)\n",
    "        \n",
    "        valid_stops['stop_cluster_id'] = stop_clusters\n",
    "        n_stop_clusters = len(set(stop_clusters)) - (1 if -1 in stop_clusters else 0)\n",
    "        \n",
    "        print(f\"Created {n_stop_clusters} stop clusters from {len(valid_stops)} stops\")\n",
    "    else:\n",
    "        print(\"No valid stop coordinates found\")\n",
    "        valid_stops = None\n",
    "else:\n",
    "    valid_stops = None\n",
    "\n",
    "print(f\"\\nSpatial Features Summary:\")\n",
    "spatial_features = [\n",
    "    'violation_cluster', 'violations_within_100m', 'stop_cluster_id'\n",
    "]\n",
    "print(f\"Created spatial intelligence features\")\n",
    "print(f\"   • {n_clusters} violation hotspot clusters\")\n",
    "print(f\"   • Density analysis for violation prediction\")\n",
    "if valid_stops is not None:\n",
    "    print(f\"   • {n_stop_clusters} stop clusters for route optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: CUNY-Specific Features\n",
    "\n",
    "creating proximity and routing features for CUNY campuses using the established coordinates and 500m buffer analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUNY-SPECIFIC FEATURE ENGINEERING\n",
      "==================================================\n",
      "Analyzing proximity to 7 CUNY campuses:\n",
      "   • Hunter College: (40.7685, -73.9656)\n",
      "   • City College: (40.82, -73.9493)\n",
      "   • Baruch College: (40.7402, -73.9836)\n",
      "   • Brooklyn College: (40.6314, -73.9521)\n",
      "   • Queens College: (40.7366, -73.817)\n",
      "   • LaGuardia CC: (40.7443, -73.9349)\n",
      "   • Bronx CC: (40.8563, -73.9125)\n",
      "\n",
      "Calculating distances to CUNY campuses...\n",
      "Computing CUNY features for 10000 violations...\n",
      "CUNY features calculated\n",
      "\n",
      "CUNY Proximity Analysis:\n",
      "Violations by nearest CUNY campus:\n",
      "   • Baruch College: 201.0 violations within 500m (avg dist: 2238m)\n",
      "   • Brooklyn College: 107.0 violations within 500m (avg dist: 3478m)\n",
      "   • City College: 44.0 violations within 500m (avg dist: 2102m)\n",
      "   • Hunter College: 36.0 violations within 500m (avg dist: 1476m)\n",
      "   • Bronx CC: 0.0 violations within 500m (avg dist: 2955m)\n",
      "   • LaGuardia CC: 0.0 violations within 500m (avg dist: 4254m)\n",
      "   • Queens College: 0.0 violations within 500m (avg dist: 3970m)\n",
      "\n",
      "CUNY Impact Summary:\n",
      "   • 388 violations within 500m of CUNY campuses\n",
      "   • 3.9% of all violations are CUNY-adjacent\n",
      "   • 388 total CUNY-area violations\n"
     ]
    }
   ],
   "source": [
    "print(\"CUNY-SPECIFIC FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# CUNY campus coordinates (from previous analysis)\n",
    "CUNY_CAMPUSES = {\n",
    "    'Hunter College': (40.7685, -73.9656),\n",
    "    'City College': (40.8200, -73.9493),\n",
    "    'Baruch College': (40.7402, -73.9836),\n",
    "    'Brooklyn College': (40.6314, -73.9521),\n",
    "    'Queens College': (40.7366, -73.8170),\n",
    "    'LaGuardia CC': (40.7443, -73.9349),\n",
    "    'Bronx CC': (40.8563, -73.9125)\n",
    "}\n",
    "\n",
    "print(f\"Analyzing proximity to {len(CUNY_CAMPUSES)} CUNY campuses:\")\n",
    "for campus, (lat, lon) in CUNY_CAMPUSES.items():\n",
    "    print(f\"   • {campus}: ({lat}, {lon})\")\n",
    "\n",
    "# calculating distance to nearest CUNY campus for each violation\n",
    "print(\"\\nCalculating distances to CUNY campuses...\")\n",
    "\n",
    "def calculate_cuny_features(df):\n",
    "    \"\"\"calculating CUNY proximity features for violations\"\"\"\n",
    "    cuny_features = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.isna(row['Violation Latitude']) or pd.isna(row['Violation Longitude']):\n",
    "            cuny_features.append({\n",
    "                'violation_id': row['Violation ID'],\n",
    "                'nearest_cuny_campus': 'Unknown',\n",
    "                'distance_to_cuny': np.nan,\n",
    "                'cuny_route_flag': False,\n",
    "                'within_cuny_500m': False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        violation_lat = row['Violation Latitude']\n",
    "        violation_lon = row['Violation Longitude']\n",
    "        \n",
    "        # calculating distance to each CUNY campus\n",
    "        distances = {}\n",
    "        for campus, (campus_lat, campus_lon) in CUNY_CAMPUSES.items():\n",
    "            distance = haversine_distance(violation_lat, violation_lon, campus_lat, campus_lon)\n",
    "            distances[campus] = distance\n",
    "        \n",
    "        # finding nearest campus\n",
    "        nearest_campus = min(distances, key=distances.get)\n",
    "        nearest_distance = distances[nearest_campus]\n",
    "        \n",
    "        # determining if within 500m buffer\n",
    "        within_500m = nearest_distance <= 500\n",
    "        \n",
    "        cuny_features.append({\n",
    "            'violation_id': row['Violation ID'],\n",
    "            'nearest_cuny_campus': nearest_campus,\n",
    "            'distance_to_cuny': nearest_distance,\n",
    "            'cuny_route_flag': within_500m,\n",
    "            'within_cuny_500m': within_500m\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(cuny_features)\n",
    "\n",
    "# calculating for a sample first (computationally intensive)\n",
    "cuny_sample_size = min(10000, len(valid_coords))\n",
    "cuny_sample = valid_coords.sample(n=cuny_sample_size, random_state=42)\n",
    "\n",
    "print(f\"Computing CUNY features for {cuny_sample_size} violations...\")\n",
    "cuny_features_df = calculate_cuny_features(cuny_sample)\n",
    "\n",
    "print(f\"CUNY features calculated\")\n",
    "\n",
    "# analyzing CUNY proximity patterns\n",
    "print(\"\\nCUNY Proximity Analysis:\")\n",
    "cuny_summary = cuny_features_df.groupby('nearest_cuny_campus').agg({\n",
    "    'violation_id': 'count',\n",
    "    'distance_to_cuny': 'mean',\n",
    "    'within_cuny_500m': 'sum'\n",
    "}).round(1)\n",
    "\n",
    "cuny_summary.columns = ['total_violations', 'avg_distance_m', 'violations_within_500m']\n",
    "cuny_summary = cuny_summary.sort_values('violations_within_500m', ascending=False)\n",
    "\n",
    "print(\"Violations by nearest CUNY campus:\")\n",
    "for campus, stats in cuny_summary.iterrows():\n",
    "    print(f\"   • {campus}: {stats['violations_within_500m']} violations within 500m (avg dist: {stats['avg_distance_m']:.0f}m)\")\n",
    "\n",
    "total_cuny_violations = cuny_features_df['within_cuny_500m'].sum()\n",
    "cuny_percentage = (total_cuny_violations / len(cuny_features_df)) * 100\n",
    "\n",
    "print(f\"\\nCUNY Impact Summary:\")\n",
    "print(f\"   • {total_cuny_violations} violations within 500m of CUNY campuses\")\n",
    "print(f\"   • {cuny_percentage:.1f}% of all violations are CUNY-adjacent\")\n",
    "print(f\"   • {cuny_summary['violations_within_500m'].sum()} total CUNY-area violations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating route-level CUNY classification...\n",
      "\n",
      "Top CUNY-serving routes:\n",
      "   • Route B44+: 89 CUNY violations (20.6%) - Brooklyn College\n",
      "   • Route M23+: 24 CUNY violations (48.0%) - Baruch College\n",
      "\n",
      "CUNY Route Statistics:\n",
      "   • 2 routes serve CUNY campuses (>20% violations within 500m)\n",
      "   • 38 routes do not primarily serve CUNY\n",
      "   • 5.0% of routes have significant CUNY service\n",
      "\n",
      "Creating campus-specific features...\n",
      "Created campus-specific distance and proximity features\n",
      "Creating CUNY temporal interaction features...\n",
      "CUNY temporal interaction features created\n",
      "\n",
      "CUNY Features Summary:\n",
      "   • Proximity to 7 campuses calculated\n",
      "   • 2 routes classified as CUNY-serving\n",
      "   • 3 temporal interaction features\n",
      "   • Campus-specific distance features for all locations\n"
     ]
    }
   ],
   "source": [
    "# creating route-level CUNY classification\n",
    "print(\"Creating route-level CUNY classification...\")\n",
    "\n",
    "# merging CUNY features back to sample violations\n",
    "cuny_enriched = cuny_sample.merge(\n",
    "    cuny_features_df[['violation_id', 'nearest_cuny_campus', 'distance_to_cuny', 'cuny_route_flag']],\n",
    "    left_on='Violation ID',\n",
    "    right_on='violation_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# route-level CUNY serving analysis\n",
    "route_cuny_analysis = cuny_enriched.groupby('Bus Route ID').agg({\n",
    "    'cuny_route_flag': ['sum', 'count', 'mean'],\n",
    "    'distance_to_cuny': 'mean',\n",
    "    'nearest_cuny_campus': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'None'\n",
    "}).round(3)\n",
    "\n",
    "route_cuny_analysis.columns = ['cuny_violations', 'total_violations', 'cuny_violation_rate', 'avg_cuny_distance', 'primary_cuny_campus']\n",
    "\n",
    "# classifying routes as CUNY-serving if >20% of violations are within 500m of campus\n",
    "route_cuny_analysis['serves_cuny_primary'] = route_cuny_analysis['cuny_violation_rate'] > 0.2\n",
    "route_cuny_analysis['cuny_intensity'] = route_cuny_analysis['cuny_violations'] / route_cuny_analysis['total_violations']\n",
    "\n",
    "# sorting by CUNY intensity\n",
    "route_cuny_analysis = route_cuny_analysis.sort_values('cuny_violations', ascending=False)\n",
    "\n",
    "print(f\"\\nTop CUNY-serving routes:\")\n",
    "cuny_routes = route_cuny_analysis[route_cuny_analysis['serves_cuny_primary']].head(10)\n",
    "for route_id, stats in cuny_routes.iterrows():\n",
    "    print(f\"   • Route {route_id}: {stats['cuny_violations']} CUNY violations ({stats['cuny_violation_rate']*100:.1f}%) - {stats['primary_cuny_campus']}\")\n",
    "\n",
    "print(f\"\\nCUNY Route Statistics:\")\n",
    "total_cuny_routes = route_cuny_analysis['serves_cuny_primary'].sum()\n",
    "total_routes = len(route_cuny_analysis)\n",
    "print(f\"   • {total_cuny_routes} routes serve CUNY campuses (>{20}% violations within 500m)\")\n",
    "print(f\"   • {total_routes - total_cuny_routes} routes do not primarily serve CUNY\")\n",
    "print(f\"   • {(total_cuny_routes/total_routes)*100:.1f}% of routes have significant CUNY service\")\n",
    "\n",
    "# creating campus-specific features\n",
    "print(\"\\nCreating campus-specific features...\")\n",
    "\n",
    "for campus in CUNY_CAMPUSES.keys():\n",
    "    # distance to specific campus\n",
    "    campus_clean = campus.replace(' ', '_').replace('College', 'C').lower()\n",
    "    cuny_enriched[f'distance_to_{campus_clean}'] = cuny_enriched.apply(\n",
    "        lambda row: haversine_distance(\n",
    "            row['Violation Latitude'], row['Violation Longitude'],\n",
    "            CUNY_CAMPUSES[campus][0], CUNY_CAMPUSES[campus][1]\n",
    "        ) if pd.notna(row['Violation Latitude']) else np.nan, axis=1\n",
    "    )\n",
    "    \n",
    "    # within 500m of specific campus\n",
    "    cuny_enriched[f'within_500m_{campus_clean}'] = cuny_enriched[f'distance_to_{campus_clean}'] <= 500\n",
    "\n",
    "print(f\"Created campus-specific distance and proximity features\")\n",
    "\n",
    "# creating CUNY temporal interaction features\n",
    "print(\"Creating CUNY temporal interaction features...\")\n",
    "\n",
    "# class change periods at CUNY campuses\n",
    "cuny_enriched['cuny_class_change_violation'] = (\n",
    "    cuny_enriched['is_cuny_class_change'] & \n",
    "    cuny_enriched['cuny_route_flag']\n",
    ")\n",
    "\n",
    "# peak student travel times (7-9am, 5-7pm) on CUNY routes\n",
    "cuny_enriched['cuny_peak_travel'] = (\n",
    "    (cuny_enriched['is_morning_rush'] | cuny_enriched['is_evening_rush']) &\n",
    "    cuny_enriched['cuny_route_flag']\n",
    ")\n",
    "\n",
    "# weekend activity near CUNY (different pattern than weekday)\n",
    "cuny_enriched['cuny_weekend_activity'] = (\n",
    "    cuny_enriched['is_weekend'] &\n",
    "    cuny_enriched['cuny_route_flag'] &\n",
    "    cuny_enriched['hour_of_day'].between(10, 18)\n",
    ")\n",
    "\n",
    "print(f\"CUNY temporal interaction features created\")\n",
    "\n",
    "cuny_interaction_features = [\n",
    "    'cuny_class_change_violation', 'cuny_peak_travel', 'cuny_weekend_activity'\n",
    "]\n",
    "\n",
    "print(f\"\\nCUNY Features Summary:\")\n",
    "print(f\"   • Proximity to {len(CUNY_CAMPUSES)} campuses calculated\")\n",
    "print(f\"   • {total_cuny_routes} routes classified as CUNY-serving\")\n",
    "print(f\"   • {len(cuny_interaction_features)} temporal interaction features\")\n",
    "print(f\"   • Campus-specific distance features for all locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Enforcement Adaptation Features\n",
    "\n",
    "building features that capture violator learning and adaptation patterns - critical for understanding the enforcement effectiveness paradox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENFORCEMENT ADAPTATION FEATURE ENGINEERING\n",
      "==================================================\n",
      "Implementing insights from previous analysis:\n",
      "   • -0.169 correlation between enforcement duration and effectiveness\n",
      "   • Violators adapt to predictable enforcement patterns\n",
      "   • Repeat offenders show learning behaviors\n",
      "\n",
      "Analyzing adaptation patterns on strategic sample...\n",
      "Processing 500,000 violations for adaptation analysis\n",
      "\n",
      "Creating location-based enforcement features...\n",
      "Location-based features created for 500,000 violations\n",
      "\n",
      "Analyzing repeat offender patterns...\n",
      "Vehicle adaptation features created for 491,270 violations\n",
      "\n",
      "Repeat Offender Analysis:\n",
      "                    violation_id  vehicle_violation_sequence  \\\n",
      "is_repeat_offender                                             \n",
      "False                     317160                        1.00   \n",
      "True                      174110                        6.64   \n",
      "\n",
      "                    days_since_last_violation  vehicle_route_switches  \n",
      "is_repeat_offender                                                     \n",
      "False                                     NaN                    0.00  \n",
      "True                                     87.8                    0.88  \n",
      "\n",
      "Repeat Offender Summary:\n",
      "   • 174,110 repeat violations (35.4% of total)\n",
      "   • 317,160 first-time violations\n",
      "   • 26,634 violations from super repeat offenders (10+ violations)\n"
     ]
    }
   ],
   "source": [
    "print(\"ENFORCEMENT ADAPTATION FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Implementing insights from previous analysis:\")\n",
    "print(\"   • Violators adapt to predictable enforcement patterns\")\n",
    "print(\"   • Repeat offenders show learning behaviors\")\n",
    "\n",
    "# working with strategic sample for adaptation analysis (much faster processing)\n",
    "print(\"\\nAnalyzing adaptation patterns on strategic sample...\")\n",
    "\n",
    "# sample 500K violations maintaining temporal distribution\n",
    "sample_size = 500000\n",
    "violations_sorted = violations_data.sample(n=sample_size, random_state=42).sort_values('violation_datetime').copy()\n",
    "\n",
    "print(f\"Processing {len(violations_sorted):,} violations for adaptation analysis\")\n",
    "\n",
    "# 1. location-based enforcement history\n",
    "print(\"\\nCreating location-based enforcement features...\")\n",
    "\n",
    "# grouping by stop_id to track enforcement at specific locations\n",
    "location_groups = violations_sorted.groupby('Stop ID')\n",
    "\n",
    "location_features = []\n",
    "for stop_id, group in location_groups:\n",
    "    group_sorted = group.sort_values('violation_datetime')\n",
    "    \n",
    "    for idx, (_, row) in enumerate(group_sorted.iterrows()):\n",
    "        # cumulative violations at this location up to this point\n",
    "        cumulative_violations = idx + 1\n",
    "        \n",
    "        # days since first violation at this location\n",
    "        if idx == 0:\n",
    "            days_since_first = 0\n",
    "        else:\n",
    "            days_since_first = (row['violation_datetime'] - group_sorted.iloc[0]['violation_datetime']).days\n",
    "        \n",
    "        # recent violation density (last 7 days)\n",
    "        recent_cutoff = row['violation_datetime'] - timedelta(days=7)\n",
    "        recent_violations = group_sorted[\n",
    "            (group_sorted['violation_datetime'] < row['violation_datetime']) &\n",
    "            (group_sorted['violation_datetime'] >= recent_cutoff)\n",
    "        ]\n",
    "        recent_violation_count = len(recent_violations)\n",
    "        \n",
    "        location_features.append({\n",
    "            'violation_id': row['Violation ID'],\n",
    "            'cumulative_violations_at_location': cumulative_violations,\n",
    "            'days_since_first_violation': days_since_first,\n",
    "            'recent_violations_7d': recent_violation_count\n",
    "        })\n",
    "\n",
    "location_adaptation_df = pd.DataFrame(location_features)\n",
    "print(f\"Location-based features created for {len(location_adaptation_df):,} violations\")\n",
    "\n",
    "# 2. vehicle-based repeat offender analysis\n",
    "print(\"\\nAnalyzing repeat offender patterns...\")\n",
    "\n",
    "vehicle_groups = violations_sorted.groupby('Vehicle ID')\n",
    "\n",
    "vehicle_features = []\n",
    "for vehicle_id, group in vehicle_groups:\n",
    "    group_sorted = group.sort_values('violation_datetime')\n",
    "    \n",
    "    for idx, (_, row) in enumerate(group_sorted.iterrows()):\n",
    "        # vehicle violation sequence number\n",
    "        violation_sequence = idx + 1\n",
    "        \n",
    "        # time since last violation by this vehicle\n",
    "        if idx == 0:\n",
    "            days_since_last_violation = np.nan\n",
    "            avg_violation_interval = np.nan\n",
    "        else:\n",
    "            days_since_last = (row['violation_datetime'] - group_sorted.iloc[idx-1]['violation_datetime']).days\n",
    "            days_since_last_violation = days_since_last\n",
    "            \n",
    "            # Average interval between violations for this vehicle\n",
    "            if idx > 1:\n",
    "                all_intervals = [(group_sorted.iloc[i]['violation_datetime'] - group_sorted.iloc[i-1]['violation_datetime']).days \n",
    "                               for i in range(1, idx+1)]\n",
    "                avg_violation_interval = np.mean(all_intervals)\n",
    "            else:\n",
    "                avg_violation_interval = days_since_last\n",
    "        \n",
    "        # route switching behavior (adaptation indicator)\n",
    "        if idx == 0:\n",
    "            route_switches = 0\n",
    "        else:\n",
    "            previous_routes = group_sorted.iloc[:idx]['Bus Route ID'].tolist()\n",
    "            route_switches = len(set(previous_routes + [row['Bus Route ID']])) - 1\n",
    "        \n",
    "        vehicle_features.append({\n",
    "            'violation_id': row['Violation ID'],\n",
    "            'vehicle_violation_sequence': violation_sequence,\n",
    "            'days_since_last_violation': days_since_last_violation,\n",
    "            'avg_violation_interval': avg_violation_interval,\n",
    "            'vehicle_route_switches': route_switches,\n",
    "            'is_repeat_offender': violation_sequence > 1\n",
    "        })\n",
    "\n",
    "vehicle_adaptation_df = pd.DataFrame(vehicle_features)\n",
    "print(f\"Vehicle adaptation features created for {len(vehicle_adaptation_df):,} violations\")\n",
    "\n",
    "# analyzing repeat offender patterns\n",
    "repeat_stats = vehicle_adaptation_df.groupby('is_repeat_offender').agg({\n",
    "    'violation_id': 'count',\n",
    "    'vehicle_violation_sequence': 'mean',\n",
    "    'days_since_last_violation': 'mean',\n",
    "    'vehicle_route_switches': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(f\"\\nRepeat Offender Analysis:\")\n",
    "print(repeat_stats)\n",
    "\n",
    "total_violations = len(vehicle_adaptation_df)\n",
    "repeat_violations = vehicle_adaptation_df['is_repeat_offender'].sum()\n",
    "repeat_percentage = (repeat_violations / total_violations) * 100\n",
    "\n",
    "print(f\"\\nRepeat Offender Summary:\")\n",
    "print(f\"   • {repeat_violations:,} repeat violations ({repeat_percentage:.1f}% of total)\")\n",
    "print(f\"   • {total_violations - repeat_violations:,} first-time violations\")\n",
    "\n",
    "# identifying super repeat offenders (10+ violations)\n",
    "super_repeaters = vehicle_adaptation_df[vehicle_adaptation_df['vehicle_violation_sequence'] >= 10]\n",
    "print(f\"   • {len(super_repeaters):,} violations from super repeat offenders (10+ violations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating enforcement predictability features...\n",
      "Predictability analysis completed for 2594 unique locations\n",
      "   Average enforcement entropy: 3.310\n",
      "   Most predictable locations (entropy < 1.0): 172\n",
      "   Most unpredictable locations (entropy > 3.0): 1887\n",
      "\n",
      "Creating repeat offender concentration features...\n"
     ]
    }
   ],
   "source": [
    "# creating enforcement predictability analysis\n",
    "print(\"\\nCreating enforcement predictability features...\")\n",
    "\n",
    "# calculating enforcement patterns by location and time\n",
    "location_time_patterns = violations_sorted.groupby(['Stop ID', 'hour_of_day']).size().reset_index(name='historical_violations')\n",
    "\n",
    "# calculating entropy of enforcement times at each location (higher = more unpredictable)\n",
    "location_entropy = []\n",
    "for stop_id in violations_sorted['Stop ID'].unique():\n",
    "    stop_violations = violations_sorted[violations_sorted['Stop ID'] == stop_id]\n",
    "    hour_counts = stop_violations['hour_of_day'].value_counts()\n",
    "    \n",
    "    if len(hour_counts) > 1:\n",
    "        # calculating entropy\n",
    "        probabilities = hour_counts / hour_counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
    "    else:\n",
    "        entropy = 0  # completely predictable\n",
    "    \n",
    "    location_entropy.append({\n",
    "        'stop_id': stop_id,\n",
    "        'enforcement_predictability': 1 / (entropy + 1),  # Higher = more predictable\n",
    "        'enforcement_entropy': entropy\n",
    "    })\n",
    "\n",
    "entropy_df = pd.DataFrame(location_entropy)\n",
    "\n",
    "print(f\"Predictability analysis completed for {len(entropy_df)} unique locations\")\n",
    "print(f\"   Average enforcement entropy: {entropy_df['enforcement_entropy'].mean():.3f}\")\n",
    "print(f\"   Most predictable locations (entropy < 1.0): {(entropy_df['enforcement_entropy'] < 1.0).sum()}\")\n",
    "print(f\"   Most unpredictable locations (entropy > 3.0): {(entropy_df['enforcement_entropy'] > 3.0).sum()}\")\n",
    "\n",
    "# creating repeat offender concentration features\n",
    "print(\"\\nCreating repeat offender concentration features...\")\n",
    "\n",
    "# calculating what percentage of violations at each location come from repeat offenders\n",
    "location_repeat_concentration = []\n",
    "\n",
    "for stop_id in violations_sorted['Stop ID'].unique():\n",
    "    stop_violations = violations_sorted[violations_sorted['Stop ID'] == stop_id]\n",
    "    \n",
    "    # counting unique vehicles and their violation counts\n",
    "    vehicle_counts = stop_violations['Vehicle ID'].value_counts()\n",
    "    \n",
    "    # calculating concentration metrics\n",
    "    total_violations = len(stop_violations)\n",
    "    unique_vehicles = len(vehicle_counts)\n",
    "    repeat_offender_violations = (vehicle_counts > 1).sum()\n",
    "    violations_from_repeaters = vehicle_counts[vehicle_counts > 1].sum()\n",
    "    \n",
    "    if total_violations > 0:\n",
    "        repeat_concentration = violations_from_repeaters / total_violations\n",
    "        vehicle_diversity = unique_vehicles / total_violations  # Lower = more concentrated\n",
    "    else:\n",
    "        repeat_concentration = 0\n",
    "        vehicle_diversity = 0\n",
    "    \n",
    "    location_repeat_concentration.append({\n",
    "        'stop_id': stop_id,\n",
    "        'repeat_offender_concentration': repeat_concentration,\n",
    "        'vehicle_diversity_index': vehicle_diversity,\n",
    "        'unique_violators': unique_vehicles,\n",
    "        'total_violations_at_stop': total_violations\n",
    "    })\n",
    "\n",
    "concentration_df = pd.DataFrame(location_repeat_concentration)\n",
    "\n",
    "print(f\"Concentration analysis completed\")\n",
    "print(f\"   Average repeat offender concentration: {concentration_df['repeat_offender_concentration'].mean():.3f}\")\n",
    "print(f\"   Locations with >50% repeat violations: {(concentration_df['repeat_offender_concentration'] > 0.5).sum()}\")\n",
    "print(f\"   Average vehicle diversity index: {concentration_df['vehicle_diversity_index'].mean():.3f}\")\n",
    "\n",
    "# detecting violation learning curves\n",
    "print(\"\\nDetecting violation learning curves...\")\n",
    "\n",
    "# for routes with enough data, detecting if violation rates decrease over time (learning effect)\n",
    "route_learning_curves = []\n",
    "\n",
    "for route_id in violations_sorted['Bus Route ID'].unique():\n",
    "    route_violations = violations_sorted[violations_sorted['Bus Route ID'] == route_id]\n",
    "    \n",
    "    if len(route_violations) >= 30:  # need enough data for trend analysis\n",
    "        # grouping by month and counting violations\n",
    "        monthly_violations = route_violations.groupby(route_violations['violation_datetime'].dt.to_period('M')).size()\n",
    "        \n",
    "        if len(monthly_violations) >= 3:  # need at least 3 months\n",
    "            # calculating trend (negative slope = decreasing violations = learning)\n",
    "            x = np.arange(len(monthly_violations))\n",
    "            y = monthly_violations.values\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            \n",
    "            route_learning_curves.append({\n",
    "                'route_id': route_id,\n",
    "                'violation_trend_slope': slope,\n",
    "                'trend_r_squared': r_value**2,\n",
    "                'trend_p_value': p_value,\n",
    "                'shows_learning_curve': (slope < -0.5) and (p_value < 0.1),  # declining trend\n",
    "                'months_of_data': len(monthly_violations),\n",
    "                'total_violations': len(route_violations)\n",
    "            })\n",
    "\n",
    "learning_df = pd.DataFrame(route_learning_curves)\n",
    "\n",
    "if len(learning_df) > 0:\n",
    "    print(f\"Learning curve analysis completed for {len(learning_df)} routes with sufficient data\")\n",
    "    learning_routes = learning_df['shows_learning_curve'].sum()\n",
    "    print(f\"   Routes showing learning curves: {learning_routes} ({(learning_routes/len(learning_df)*100):.1f}%)\")\n",
    "    print(f\"   Average trend slope: {learning_df['violation_trend_slope'].mean():.3f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for learning curve analysis\")\n",
    "\n",
    "print(f\"\\nEnforcement Adaptation Features Summary:\")\n",
    "adaptation_features = [\n",
    "    'cumulative_violations_at_location', 'days_since_first_violation',\n",
    "    'vehicle_violation_sequence', 'is_repeat_offender', 'vehicle_route_switches',\n",
    "    'enforcement_predictability', 'repeat_offender_concentration',\n",
    "    'vehicle_diversity_index', 'violation_trend_slope'\n",
    "]\n",
    "print(f\"Created {len(adaptation_features)} adaptation features\")\n",
    "print(f\"   • Location learning: cumulative violations, time since first\")\n",
    "print(f\"   • Vehicle adaptation: repeat patterns, route switching\")\n",
    "print(f\"   • Predictability: enforcement entropy, concentration\")\n",
    "print(f\"   • Learning curves: trend analysis for {len(learning_df) if len(learning_df) > 0 else 0} routes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Target Variable Engineering\n",
    "\n",
    "creating multiple prediction targets for different use cases:\n",
    "- immediate prediction (next hour)\n",
    "- tactical planning (next day)\n",
    "- strategic forecasting (binary classification)\n",
    "- speed impact correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET VARIABLE ENGINEERING\n",
      "==================================================\n",
      "Creating multiple prediction targets for different operational scenarios\n",
      "\n",
      "Aggregating violations by location and time...\n",
      "Created 453,935 unique location-hour combinations\n",
      "   Date range: 2019-10-07 07:00:00 to 2025-08-21 17:00:00\n",
      "   Average violations per location-hour: 1.10\n",
      "   Max violations in one location-hour: 9\n",
      "\n",
      "Creating target variables...\n",
      "Target 1: violation_count_next_hour (immediate prediction)\n",
      "Next hour targets created for 451,416 observations\n",
      "   Non-null targets: 451,341\n",
      "Target 2: violation_count_next_day (tactical planning)\n",
      "Next day targets created for 264,513 daily observations\n",
      "   Average daily violations: 1.88\n",
      "Target 3: high_violation_flag (binary classification)\n",
      "   High violation threshold: >1 violations per hour\n",
      "High violation flags created: 39,320 high periods (8.7%)\n",
      "Target 4: Multiple severity thresholds\n",
      "   moderate_violation_flag: 39,320 periods (8.7%) above 1.0 violations\n",
      "   severe_violation_flag: 39,320 periods (8.7%) above 1.0 violations\n",
      "   extreme_violation_flag: 5,319 periods (1.2%) above 2.0 violations\n",
      "Multiple severity targets created\n"
     ]
    }
   ],
   "source": [
    "print(\"TARGET VARIABLE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Creating multiple prediction targets for different operational scenarios\")\n",
    "\n",
    "# group violations by location and time for target creation\n",
    "print(\"\\nAggregating violations by location and time...\")\n",
    "\n",
    "# create unique location-time combinations\n",
    "modeling_groups = violations_sorted.groupby(['Stop ID', 'violation_hour']).agg({\n",
    "    'Violation ID': 'count',\n",
    "    'Vehicle ID': 'nunique',\n",
    "    'Bus Route ID': 'first',  # primary route for this stop\n",
    "    'Violation Latitude': 'first',\n",
    "    'Violation Longitude': 'first',\n",
    "    'Stop Name': 'first',\n",
    "    'Bus Stop Latitude': 'first',\n",
    "    'Bus Stop Longitude': 'first',\n",
    "    'hour_of_day': 'first',\n",
    "    'day_of_week': 'first',\n",
    "    'month': 'first',\n",
    "    'is_weekend': 'first',\n",
    "    'rush_hour_period': 'first',\n",
    "    'is_school_hours': 'first',\n",
    "    'semester_period': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# renaming violation count column\n",
    "modeling_groups = modeling_groups.rename(columns={'Violation ID': 'violation_count'})\n",
    "\n",
    "print(f\"Created {len(modeling_groups):,} unique location-hour combinations\")\n",
    "print(f\"   Date range: {modeling_groups['violation_hour'].min()} to {modeling_groups['violation_hour'].max()}\")\n",
    "print(f\"   Average violations per location-hour: {modeling_groups['violation_count'].mean():.2f}\")\n",
    "print(f\"   Max violations in one location-hour: {modeling_groups['violation_count'].max()}\")\n",
    "\n",
    "# sorting by time for target engineering\n",
    "modeling_groups = modeling_groups.sort_values(['Stop ID', 'violation_hour'])\n",
    "\n",
    "print(\"\\nCreating target variables...\")\n",
    "\n",
    "# 1. next hour prediction target\n",
    "print(\"Target 1: violation_count_next_hour (immediate prediction)\")\n",
    "\n",
    "next_hour_targets = []\n",
    "for stop_id in modeling_groups['Stop ID'].unique():\n",
    "    stop_data = modeling_groups[modeling_groups['Stop ID'] == stop_id].copy()\n",
    "    stop_data = stop_data.sort_values('violation_hour')\n",
    "    \n",
    "    # creating next hour targets\n",
    "    stop_data['violation_count_next_hour'] = stop_data['violation_count'].shift(-1)\n",
    "    \n",
    "    # removing last row (no future data)\n",
    "    stop_data = stop_data[:-1] if len(stop_data) > 1 else stop_data\n",
    "    \n",
    "    next_hour_targets.append(stop_data)\n",
    "\n",
    "if next_hour_targets:\n",
    "    next_hour_df = pd.concat(next_hour_targets, ignore_index=True)\n",
    "    print(f\"Next hour targets created for {len(next_hour_df):,} observations\")\n",
    "    print(f\"   Non-null targets: {next_hour_df['violation_count_next_hour'].notna().sum():,}\")\n",
    "else:\n",
    "    next_hour_df = modeling_groups.copy()\n",
    "    next_hour_df['violation_count_next_hour'] = np.nan\n",
    "\n",
    "# 2. next day prediction target\n",
    "print(\"Target 2: violation_count_next_day (tactical planning)\")\n",
    "\n",
    "# Aggregate to daily level first\n",
    "daily_groups = violations_sorted.groupby(['Stop ID', violations_sorted['violation_datetime'].dt.date]).agg({\n",
    "    'Violation ID': 'count',\n",
    "    'Vehicle ID': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "daily_groups = daily_groups.rename(columns={\n",
    "    'violation_datetime': 'violation_date',\n",
    "    'Violation ID': 'daily_violation_count',\n",
    "    'Vehicle ID': 'daily_unique_vehicles'\n",
    "})\n",
    "\n",
    "# creating next day targets\n",
    "next_day_targets = []\n",
    "for stop_id in daily_groups['Stop ID'].unique():\n",
    "    stop_daily = daily_groups[daily_groups['Stop ID'] == stop_id].copy()\n",
    "    stop_daily = stop_daily.sort_values('violation_date')\n",
    "    \n",
    "    stop_daily['violation_count_next_day'] = stop_daily['daily_violation_count'].shift(-1)\n",
    "    stop_daily['unique_vehicles_next_day'] = stop_daily['daily_unique_vehicles'].shift(-1)\n",
    "    \n",
    "    # removing last row\n",
    "    stop_daily = stop_daily[:-1] if len(stop_daily) > 1 else stop_daily\n",
    "    \n",
    "    next_day_targets.append(stop_daily)\n",
    "\n",
    "if next_day_targets:\n",
    "    next_day_df = pd.concat(next_day_targets, ignore_index=True)\n",
    "    print(f\"Next day targets created for {len(next_day_df):,} daily observations\")\n",
    "    print(f\"   Average daily violations: {next_day_df['daily_violation_count'].mean():.2f}\")\n",
    "else:\n",
    "    next_day_df = pd.DataFrame()\n",
    "\n",
    "# creating binary high violation flag\n",
    "print(\"Target 3: high_violation_flag (binary classification)\")\n",
    "\n",
    "# defining threshold for \"high violation\" periods\n",
    "violation_threshold = modeling_groups['violation_count'].quantile(0.8)  # top 20%\n",
    "print(f\"   High violation threshold: >{violation_threshold:.0f} violations per hour\")\n",
    "\n",
    "modeling_groups['high_violation_flag'] = (modeling_groups['violation_count'] > violation_threshold).astype(int)\n",
    "\n",
    "high_violation_count = modeling_groups['high_violation_flag'].sum()\n",
    "high_violation_pct = (high_violation_count / len(modeling_groups)) * 100\n",
    "\n",
    "print(f\"High violation flags created: {high_violation_count:,} high periods ({high_violation_pct:.1f}%)\")\n",
    "\n",
    "# creating multiple threshold targets for different severity levels\n",
    "print(\"Target 4: Multiple severity thresholds\")\n",
    "\n",
    "thresholds = {\n",
    "    'moderate_violation_flag': modeling_groups['violation_count'].quantile(0.6),  # Top 40%\n",
    "    'severe_violation_flag': modeling_groups['violation_count'].quantile(0.9),   # Top 10%\n",
    "    'extreme_violation_flag': modeling_groups['violation_count'].quantile(0.95)  # Top 5%\n",
    "}\n",
    "\n",
    "for flag_name, threshold in thresholds.items():\n",
    "    modeling_groups[flag_name] = (modeling_groups['violation_count'] > threshold).astype(int)\n",
    "    count = modeling_groups[flag_name].sum()\n",
    "    pct = (count / len(modeling_groups)) * 100\n",
    "    print(f\"   {flag_name}: {count:,} periods ({pct:.1f}%) above {threshold:.1f} violations\")\n",
    "\n",
    "print(f\"Multiple severity targets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target 5: speed_impact_score (enforcement effectiveness)\n",
      "   loading speed datasets for correlation analysis...\n",
      "   Loading historical_early speeds...\n",
      "   Loading historical_recent speeds...\n",
      "   Loading current speeds...\n",
      "Loaded 108,657 speed records for correlation\n",
      "   Speed changes calculated for 524 routes\n",
      "   Routes with speed improvement: 70\n",
      "   Average speed change: -2.57%\n",
      "Speed impact scores calculated for 38 routes\n",
      "   Best performing route: BX19\n",
      "   Worst performing route: Q44+\n",
      "\n",
      "Target 6: Composite prediction targets\n",
      "Composite targets created:\n",
      "   Violation risk score range: 0.4 - 19.6\n",
      "   Deployment priority range: 0.3 - 6.0\n",
      "\n",
      "TARGET VARIABLES SUMMARY:\n",
      "Created 9 target variables for different prediction scenarios:\n",
      "   • Immediate forecasting: violation_count_next_hour\n",
      "   • Tactical planning: violation_count_next_day\n",
      "   • Binary classification: high/moderate/severe/extreme violation flags\n",
      "   • Effectiveness measurement: speed_impact_score\n",
      "   • Operational optimization: violation_risk_score, deployment_priority\n"
     ]
    }
   ],
   "source": [
    "# creating speed impact score target\n",
    "print(\"Target 5: speed_impact_score (enforcement effectiveness)\")\n",
    "\n",
    "# loading speed data to correlate with violations\n",
    "print(\"   loading speed datasets for correlation analysis...\")\n",
    "\n",
    "speed_files = {\n",
    "    '../data/MTA_Bus_Speeds__2015-2019_20250919.csv': 'historical_early',\n",
    "    '../data/MTA_Bus_Speeds__2020_-_2024_20250919.csv': 'historical_recent', \n",
    "    '../data/MTA_Bus_Speeds__Beginning_2025_20250919.csv': 'current'\n",
    "}\n",
    "\n",
    "all_speeds = []\n",
    "for file_path, period in speed_files.items():\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"   Loading {period} speeds...\")\n",
    "        \n",
    "        # loading sample for analysis (full files are very large)\n",
    "        speed_df = pd.read_csv(file_path, nrows=50000)  # sample for efficiency\n",
    "        speed_df['period'] = period\n",
    "        speed_df['month_date'] = pd.to_datetime(speed_df['month'], errors='coerce')\n",
    "        \n",
    "        all_speeds.append(speed_df)\n",
    "\n",
    "if all_speeds:\n",
    "    combined_speeds = pd.concat(all_speeds, ignore_index=True)\n",
    "    print(f\"Loaded {len(combined_speeds):,} speed records for correlation\")\n",
    "    \n",
    "    # calculating route-level speed changes (ACE implementation impact)\n",
    "    ace_cutoff = datetime(2024, 6, 1)\n",
    "    combined_speeds['is_post_ace'] = combined_speeds['month_date'] >= ace_cutoff\n",
    "    \n",
    "    # grouping by route and calculating pre/post ACE speed changes\n",
    "    route_speed_changes = combined_speeds.groupby(['route_id', 'is_post_ace'])['average_speed'].mean().unstack(fill_value=np.nan)\n",
    "    \n",
    "    if True in route_speed_changes.columns and False in route_speed_changes.columns:\n",
    "        route_speed_changes['speed_change_pct'] = (\n",
    "            (route_speed_changes[True] - route_speed_changes[False]) / \n",
    "            route_speed_changes[False] * 100\n",
    "        )\n",
    "        route_speed_changes['speed_improved'] = route_speed_changes['speed_change_pct'] > 0\n",
    "        \n",
    "        print(f\"   Speed changes calculated for {len(route_speed_changes)} routes\")\n",
    "        print(f\"   Routes with speed improvement: {route_speed_changes['speed_improved'].sum()}\")\n",
    "        print(f\"   Average speed change: {route_speed_changes['speed_change_pct'].mean():.2f}%\")\n",
    "        \n",
    "        # creating speed impact score for routes with both violation and speed data\n",
    "        route_violations = modeling_groups.groupby('Bus Route ID').agg({\n",
    "            'violation_count': 'sum',\n",
    "            'Vehicle ID': 'sum'  # total unique vehicles\n",
    "        }).reset_index()\n",
    "        \n",
    "        # merging violations with speed changes\n",
    "        speed_violation_correlation = route_violations.merge(\n",
    "            route_speed_changes[['speed_change_pct', 'speed_improved']].reset_index(),\n",
    "            left_on='Bus Route ID',\n",
    "            right_on='route_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        if len(speed_violation_correlation) > 0:\n",
    "            # calculating speed impact score: violations weighted by speed change\n",
    "            speed_violation_correlation['speed_impact_score'] = (\n",
    "                speed_violation_correlation['violation_count'] * \n",
    "                speed_violation_correlation['speed_change_pct']\n",
    "            )\n",
    "            \n",
    "            # normalizing to 0-1 scale\n",
    "            min_score = speed_violation_correlation['speed_impact_score'].min()\n",
    "            max_score = speed_violation_correlation['speed_impact_score'].max()\n",
    "            \n",
    "            if max_score != min_score:\n",
    "                speed_violation_correlation['speed_impact_normalized'] = (\n",
    "                    (speed_violation_correlation['speed_impact_score'] - min_score) / \n",
    "                    (max_score - min_score)\n",
    "                )\n",
    "            else:\n",
    "                speed_violation_correlation['speed_impact_normalized'] = 0.5\n",
    "            \n",
    "            print(f\"Speed impact scores calculated for {len(speed_violation_correlation)} routes\")\n",
    "            print(f\"   Best performing route: {speed_violation_correlation.loc[speed_violation_correlation['speed_impact_score'].idxmax(), 'Bus Route ID']}\")\n",
    "            print(f\"   Worst performing route: {speed_violation_correlation.loc[speed_violation_correlation['speed_impact_score'].idxmin(), 'Bus Route ID']}\")\n",
    "        else:\n",
    "            print(\"No matching routes between violations and speed data\")\n",
    "            speed_violation_correlation = pd.DataFrame()\n",
    "    else:\n",
    "        print(\"Insufficient speed data for pre/post ACE comparison\")\n",
    "        speed_violation_correlation = pd.DataFrame()\n",
    "else:\n",
    "    print(\"No speed data available for correlation analysis\")\n",
    "    speed_violation_correlation = pd.DataFrame()\n",
    "\n",
    "# creating composite prediction targets\n",
    "print(\"\\nTarget 6: Composite prediction targets\")\n",
    "\n",
    "# risk score: combination of violation count and severity flags\n",
    "modeling_groups['violation_risk_score'] = (\n",
    "    modeling_groups['violation_count'] * 0.4 +\n",
    "    modeling_groups['high_violation_flag'] * 3 +\n",
    "    modeling_groups['severe_violation_flag'] * 5 +\n",
    "    modeling_groups['extreme_violation_flag'] * 8\n",
    ")\n",
    "\n",
    "# deployment priority score (higher = more urgent)\n",
    "modeling_groups['deployment_priority'] = (\n",
    "    modeling_groups['violation_count'] * 0.3 +\n",
    "    modeling_groups['Vehicle ID'] * 0.2 +  # unique vehicles\n",
    "    modeling_groups['high_violation_flag'] * 2\n",
    ")\n",
    "\n",
    "print(f\"Composite targets created:\")\n",
    "print(f\"   Violation risk score range: {modeling_groups['violation_risk_score'].min():.1f} - {modeling_groups['violation_risk_score'].max():.1f}\")\n",
    "print(f\"   Deployment priority range: {modeling_groups['deployment_priority'].min():.1f} - {modeling_groups['deployment_priority'].max():.1f}\")\n",
    "\n",
    "print(f\"\\nTARGET VARIABLES SUMMARY:\")\n",
    "target_variables = [\n",
    "    'violation_count_next_hour',\n",
    "    'violation_count_next_day', \n",
    "    'high_violation_flag',\n",
    "    'moderate_violation_flag',\n",
    "    'severe_violation_flag',\n",
    "    'extreme_violation_flag',\n",
    "    'speed_impact_score',\n",
    "    'violation_risk_score',\n",
    "    'deployment_priority'\n",
    "]\n",
    "\n",
    "print(f\"Created {len(target_variables)} target variables for different prediction scenarios:\")\n",
    "print(f\"   • Immediate forecasting: violation_count_next_hour\")\n",
    "print(f\"   • Tactical planning: violation_count_next_day\")\n",
    "print(f\"   • Binary classification: high/moderate/severe/extreme violation flags\")\n",
    "print(f\"   • Effectiveness measurement: speed_impact_score\")\n",
    "print(f\"   • Operational optimization: violation_risk_score, deployment_priority\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Final Dataset Assembly and Export\n",
    "\n",
    "combining all engineered features into the final modeling dataset and exporting in optimized formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATASET ASSEMBLY\n",
      "==================================================\n",
      "Base dataset: 453,935 location-hour observations\n",
      "Merging enforcement adaptation features...\n",
      "Location adaptation features merged\n",
      "Vehicle adaptation features merged\n",
      "Predictability features merged\n",
      "Concentration features merged\n",
      "Merging CUNY proximity features...\n",
      "CUNY features added for all stops\n",
      "Speed impact features merged for routes with speed data\n",
      "\n",
      "Final dataset shape: (453935, 41)\n",
      "   Observations: 453,935\n",
      "   Features: 41\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL DATASET ASSEMBLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# start with the main modeling groups dataset\n",
    "final_dataset = modeling_groups.copy()\n",
    "\n",
    "print(f\"Base dataset: {len(final_dataset):,} location-hour observations\")\n",
    "\n",
    "# merge adaptation features\n",
    "print(\"Merging enforcement adaptation features...\")\n",
    "if len(location_adaptation_df) > 0:\n",
    "    # aggregate location features to hourly level\n",
    "    location_hourly = violations_sorted.merge(\n",
    "        location_adaptation_df,\n",
    "        left_on='Violation ID',\n",
    "        right_on='violation_id',\n",
    "        how='left'\n",
    "    ).groupby(['Stop ID', 'violation_hour']).agg({\n",
    "        'cumulative_violations_at_location': 'mean',\n",
    "        'days_since_first_violation': 'mean',\n",
    "        'recent_violations_7d': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    final_dataset = final_dataset.merge(\n",
    "        location_hourly,\n",
    "        on=['Stop ID', 'violation_hour'],\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"Location adaptation features merged\")\n",
    "\n",
    "if len(vehicle_adaptation_df) > 0:\n",
    "    # aggregating vehicle features to hourly level\n",
    "    vehicle_hourly = violations_sorted.merge(\n",
    "        vehicle_adaptation_df,\n",
    "        left_on='Violation ID',\n",
    "        right_on='violation_id',\n",
    "        how='left'\n",
    "    ).groupby(['Stop ID', 'violation_hour']).agg({\n",
    "        'vehicle_violation_sequence': 'mean',\n",
    "        'is_repeat_offender': 'mean',  # percentage of repeat offenders\n",
    "        'vehicle_route_switches': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    final_dataset = final_dataset.merge(\n",
    "        vehicle_hourly,\n",
    "        on=['Stop ID', 'violation_hour'],\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"Vehicle adaptation features merged\")\n",
    "\n",
    "# merging predictability features\n",
    "if len(entropy_df) > 0:\n",
    "    final_dataset = final_dataset.merge(\n",
    "        entropy_df,\n",
    "        left_on='Stop ID',\n",
    "        right_on='stop_id',\n",
    "        how='left'\n",
    "    ).drop('stop_id', axis=1)\n",
    "    print(f\"Predictability features merged\")\n",
    "\n",
    "if len(concentration_df) > 0:\n",
    "    final_dataset = final_dataset.merge(\n",
    "        concentration_df,\n",
    "        left_on='Stop ID',\n",
    "        right_on='stop_id',\n",
    "        how='left'\n",
    "    ).drop('stop_id', axis=1)\n",
    "    print(f\"Concentration features merged\")\n",
    "\n",
    "# merging CUNY proximity features\n",
    "print(\"Merging CUNY proximity features...\")\n",
    "if len(cuny_features_df) > 0:\n",
    "    # calculating CUNY features for all stops based on coordinates\n",
    "    cuny_stop_features = []\n",
    "    \n",
    "    for _, row in final_dataset.iterrows():\n",
    "        if pd.notna(row['Violation Latitude']) and pd.notna(row['Violation Longitude']):\n",
    "            distances = {}\n",
    "            for campus, (campus_lat, campus_lon) in CUNY_CAMPUSES.items():\n",
    "                distance = haversine_distance(\n",
    "                    row['Violation Latitude'], row['Violation Longitude'],\n",
    "                    campus_lat, campus_lon\n",
    "                )\n",
    "                distances[campus] = distance\n",
    "            \n",
    "            nearest_campus = min(distances, key=distances.get)\n",
    "            nearest_distance = distances[nearest_campus]\n",
    "            \n",
    "            cuny_stop_features.append({\n",
    "                'stop_id': row['Stop ID'],\n",
    "                'nearest_cuny_campus': nearest_campus,\n",
    "                'distance_to_cuny': nearest_distance,\n",
    "                'cuny_route_flag': nearest_distance <= 500\n",
    "            })\n",
    "        else:\n",
    "            cuny_stop_features.append({\n",
    "                'stop_id': row['Stop ID'],\n",
    "                'nearest_cuny_campus': 'Unknown',\n",
    "                'distance_to_cuny': np.nan,\n",
    "                'cuny_route_flag': False\n",
    "            })\n",
    "    \n",
    "    cuny_stop_df = pd.DataFrame(cuny_stop_features).drop_duplicates('stop_id')\n",
    "    \n",
    "    final_dataset = final_dataset.merge(\n",
    "        cuny_stop_df,\n",
    "        left_on='Stop ID',\n",
    "        right_on='stop_id',\n",
    "        how='left'\n",
    "    ).drop('stop_id', axis=1)\n",
    "    \n",
    "    print(f\"CUNY features added for all stops\")\n",
    "\n",
    "# adding speed correlation features for matching routes\n",
    "if len(speed_violation_correlation) > 0:\n",
    "    final_dataset = final_dataset.merge(\n",
    "        speed_violation_correlation[['Bus Route ID', 'speed_change_pct', 'speed_improved', 'speed_impact_normalized']],\n",
    "        on='Bus Route ID',\n",
    "        how='left'\n",
    "    )\n",
    "    print(f\"Speed impact features merged for routes with speed data\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {final_dataset.shape}\")\n",
    "print(f\"   Observations: {len(final_dataset):,}\")\n",
    "print(f\"   Features: {len(final_dataset.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATA QUALITY CHECKS\n",
      "==================================================\n",
      "Missing values summary:\n",
      "No columns with excessive missing values\n",
      "\n",
      "Imputing remaining missing values...\n",
      "Missing value imputation complete\n",
      "No duplicate observations found\n",
      "\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "==================================================\n",
      "   Temporal: 7 features\n",
      "   Spatial: 3 features\n",
      "   CUNY: 3 features\n",
      "   Adaptation: 3 features\n",
      "   Targets: 4 features\n",
      "\n",
      "FINAL DATASET READY:\n",
      "   Shape: (453935, 41)\n",
      "   Time range: 2019-10-07 07:00:00 to 2025-08-21 17:00:00\n",
      "   Unique stops: 2,594\n",
      "   Unique routes: 40\n",
      "   Memory usage: 246.1 MB\n"
     ]
    }
   ],
   "source": [
    "# performing final data quality checks\n",
    "print(\"FINAL DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# checking for missing values\n",
    "missing_summary = final_dataset.isnull().sum()\n",
    "missing_pct = (missing_summary / len(final_dataset)) * 100\n",
    "\n",
    "print(\"Missing values summary:\")\n",
    "high_missing = missing_pct[missing_pct > 10]\n",
    "if len(high_missing) > 0:\n",
    "    print(\"Columns with >10% missing:\")\n",
    "    for col, pct in high_missing.items():\n",
    "        print(f\"   {col}: {pct:.1f}%\")\n",
    "else:\n",
    "    print(\"No columns with excessive missing values\")\n",
    "\n",
    "# removing or imputing highly missing columns\n",
    "columns_to_drop = missing_pct[missing_pct > 50].index.tolist()\n",
    "if columns_to_drop:\n",
    "    print(f\"\\nDropping columns with >50% missing: {columns_to_drop}\")\n",
    "    final_dataset = final_dataset.drop(columns=columns_to_drop)\n",
    "\n",
    "# filling remaining missing values\n",
    "print(\"\\nImputing remaining missing values...\")\n",
    "\n",
    "# numeric columns: fill with median\n",
    "numeric_columns = final_dataset.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    if final_dataset[col].isnull().any():\n",
    "        median_val = final_dataset[col].median()\n",
    "        final_dataset[col] = final_dataset[col].fillna(median_val)\n",
    "\n",
    "# categorical columns: fill with mode or 'Unknown'\n",
    "categorical_columns = final_dataset.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_columns:\n",
    "    if final_dataset[col].isnull().any():\n",
    "        if col in ['nearest_cuny_campus', 'rush_hour_period', 'semester_period']:\n",
    "            final_dataset[col] = final_dataset[col].fillna('Unknown')\n",
    "        else:\n",
    "            mode_val = final_dataset[col].mode()\n",
    "            if len(mode_val) > 0:\n",
    "                final_dataset[col] = final_dataset[col].fillna(mode_val.iloc[0])\n",
    "            else:\n",
    "                final_dataset[col] = final_dataset[col].fillna('Unknown')\n",
    "\n",
    "# boolean columns: fill with False\n",
    "boolean_columns = final_dataset.select_dtypes(include=['bool']).columns\n",
    "for col in boolean_columns:\n",
    "    if final_dataset[col].isnull().any():\n",
    "        final_dataset[col] = final_dataset[col].fillna(False)\n",
    "\n",
    "print(f\"Missing value imputation complete\")\n",
    "\n",
    "# removing duplicate rows\n",
    "initial_len = len(final_dataset)\n",
    "final_dataset = final_dataset.drop_duplicates(subset=['Stop ID', 'violation_hour'])\n",
    "duplicates_removed = initial_len - len(final_dataset)\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"Removed {duplicates_removed} duplicate observations\")\n",
    "else:\n",
    "    print(f\"No duplicate observations found\")\n",
    "\n",
    "# feature engineering summary\n",
    "print(f\"\\nFEATURE ENGINEERING SUMMARY\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "feature_categories = {\n",
    "    'Temporal': ['hour_of_day', 'day_of_week', 'month', 'is_weekend', 'rush_hour_period', 'is_school_hours', 'semester_period'],\n",
    "    'Spatial': ['Violation Latitude', 'Violation Longitude', 'Stop Name'],\n",
    "    'CUNY': ['nearest_cuny_campus', 'distance_to_cuny', 'cuny_route_flag'],\n",
    "    'Adaptation': ['cumulative_violations_at_location', 'is_repeat_offender', 'enforcement_predictability'],\n",
    "    'Targets': ['violation_count', 'high_violation_flag', 'violation_risk_score', 'deployment_priority']\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    available_features = [f for f in features if f in final_dataset.columns]\n",
    "    print(f\"   {category}: {len(available_features)} features\")\n",
    "\n",
    "print(f\"\\nFINAL DATASET READY:\")\n",
    "print(f\"   Shape: {final_dataset.shape}\")\n",
    "print(f\"   Time range: {final_dataset['violation_hour'].min()} to {final_dataset['violation_hour'].max()}\")\n",
    "print(f\"   Unique stops: {final_dataset['Stop ID'].nunique():,}\")\n",
    "print(f\"   Unique routes: {final_dataset['Bus Route ID'].nunique()}\")\n",
    "print(f\"   Memory usage: {final_dataset.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPORTING FINAL MODELING DATASET\n",
      "==================================================\n",
      "Exporting to Parquet: ..\\data\\processed\\modeling_dataset.parquet\n",
      "Parquet export complete: 6.8 MB\n",
      "Exporting sample to CSV: ..\\data\\processed\\modeling_dataset_sample.csv\n",
      "CSV sample export complete: 10,000 rows, 3.3 MB\n",
      "Exporting metadata: ..\\data\\processed\\dataset_metadata.json\n",
      "Metadata export complete\n",
      "Exporting feature list: ..\\data\\processed\\feature_list.txt\n",
      "Feature list export complete: 32 modeling features identified\n",
      "\n",
      "FEATURE ENGINEERING COMPLETE!\n",
      "==================================================\n",
      "Final dataset: 453,935 observations × 41 features\n",
      "Exported to: ..\\data\\processed\n",
      "   • modeling_dataset.parquet (production-ready)\n",
      "   • modeling_dataset_sample.csv (validation)\n",
      "   • dataset_metadata.json (comprehensive info)\n",
      "   • feature_list.txt (model preparation)\n",
      "\n",
      "READY FOR PREDICTIVE MODELING!\n",
      "   Next step: Build machine learning models for violation hotspot prediction\n",
      "   Prediction targets available: immediate, tactical, binary classification, risk scoring\n",
      "   Features ready: temporal, spatial, CUNY, adaptation, enforcement intelligence\n"
     ]
    }
   ],
   "source": [
    "# exporting final modeling dataset\n",
    "print(\"EXPORTING FINAL MODELING DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ensuring output directory exists\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. export as optimized Parquet (recommended for production)\n",
    "parquet_file = output_dir / \"modeling_dataset.parquet\"\n",
    "print(f\"Exporting to Parquet: {parquet_file}\")\n",
    "\n",
    "try:\n",
    "    final_dataset.to_parquet(parquet_file, index=False, compression='snappy')\n",
    "    file_size_mb = parquet_file.stat().st_size / 1024 / 1024\n",
    "    print(f\"Parquet export complete: {file_size_mb:.1f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"Parquet export failed: {e}\")\n",
    "    print(\"   Trying alternative compression...\")\n",
    "    try:\n",
    "        final_dataset.to_parquet(parquet_file, index=False)\n",
    "        print(f\"Parquet export complete (uncompressed)\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Parquet export failed: {e2}\")\n",
    "\n",
    "# exporting sample as CSV for validation\n",
    "csv_sample_file = output_dir / \"modeling_dataset_sample.csv\"\n",
    "print(f\"Exporting sample to CSV: {csv_sample_file}\")\n",
    "\n",
    "sample_size = min(10000, len(final_dataset))\n",
    "sample_dataset = final_dataset.sample(n=sample_size, random_state=42)\n",
    "sample_dataset.to_csv(csv_sample_file, index=False)\n",
    "\n",
    "file_size_mb = csv_sample_file.stat().st_size / 1024 / 1024\n",
    "print(f\"CSV sample export complete: {sample_size:,} rows, {file_size_mb:.1f} MB\")\n",
    "\n",
    "# exporting metadata and feature descriptions\n",
    "metadata_file = output_dir / \"dataset_metadata.json\"\n",
    "print(f\"Exporting metadata: {metadata_file}\")\n",
    "\n",
    "metadata = {\n",
    "    'dataset_info': {\n",
    "        'creation_date': datetime.now().isoformat(),\n",
    "        'total_observations': len(final_dataset),\n",
    "        'total_features': len(final_dataset.columns),\n",
    "        'date_range': {\n",
    "            'start': final_dataset['violation_hour'].min().isoformat(),\n",
    "            'end': final_dataset['violation_hour'].max().isoformat()\n",
    "        },\n",
    "        'spatial_coverage': {\n",
    "            'unique_stops': final_dataset['Stop ID'].nunique(),\n",
    "            'unique_routes': final_dataset['Bus Route ID'].nunique()\n",
    "        }\n",
    "    },\n",
    "    'feature_categories': {\n",
    "        'temporal_features': [col for col in final_dataset.columns if any(keyword in col.lower() for keyword in ['hour', 'day', 'month', 'weekend', 'rush', 'school', 'semester'])],\n",
    "        'spatial_features': [col for col in final_dataset.columns if any(keyword in col.lower() for keyword in ['latitude', 'longitude', 'stop', 'distance'])],\n",
    "        'cuny_features': [col for col in final_dataset.columns if 'cuny' in col.lower()],\n",
    "        'adaptation_features': [col for col in final_dataset.columns if any(keyword in col.lower() for keyword in ['cumulative', 'repeat', 'predictability', 'concentration'])],\n",
    "        'target_variables': [col for col in final_dataset.columns if any(keyword in col.lower() for keyword in ['flag', 'score', 'priority', 'next'])]\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'missing_values_summary': final_dataset.isnull().sum().to_dict(),\n",
    "        'duplicate_rows_removed': duplicates_removed,\n",
    "        'memory_usage_mb': round(final_dataset.memory_usage(deep=True).sum() / 1024 / 1024, 2)\n",
    "    },\n",
    "    'target_variable_distributions': {\n",
    "        'violation_count_stats': {\n",
    "            'mean': float(final_dataset['violation_count'].mean()),\n",
    "            'median': float(final_dataset['violation_count'].median()),\n",
    "            'max': int(final_dataset['violation_count'].max()),\n",
    "            'std': float(final_dataset['violation_count'].std())\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# adding binary target distributions if they exist\n",
    "binary_targets = ['high_violation_flag', 'moderate_violation_flag', 'severe_violation_flag']\n",
    "for target in binary_targets:\n",
    "    if target in final_dataset.columns:\n",
    "        metadata['target_variable_distributions'][f'{target}_distribution'] = {\n",
    "            'positive_cases': int(final_dataset[target].sum()),\n",
    "            'negative_cases': int((final_dataset[target] == 0).sum()),\n",
    "            'positive_rate': float(final_dataset[target].mean())\n",
    "        }\n",
    "\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Metadata export complete\")\n",
    "\n",
    "# exporting feature importance for model preparation\n",
    "features_file = output_dir / \"feature_list.txt\"\n",
    "print(f\"Exporting feature list: {features_file}\")\n",
    "\n",
    "# excluding ID and target columns for modeling\n",
    "modeling_features = [col for col in final_dataset.columns if col not in [\n",
    "    'Stop ID', 'violation_hour', 'Violation ID', 'Vehicle ID',\n",
    "    'violation_count_next_hour', 'violation_count_next_day',\n",
    "    'high_violation_flag', 'moderate_violation_flag', 'severe_violation_flag', 'extreme_violation_flag',\n",
    "    'violation_risk_score', 'deployment_priority'\n",
    "]]\n",
    "\n",
    "with open(features_file, 'w') as f:\n",
    "    f.write(\"# ACE Intelligence System - Modeling Features\\n\")\n",
    "    f.write(f\"# Generated: {datetime.now().isoformat()}\\n\")\n",
    "    f.write(f\"# Total features: {len(modeling_features)}\\n\\n\")\n",
    "    \n",
    "    for category, features in feature_categories.items():\n",
    "        available_features = [f for f in features if f in modeling_features]\n",
    "        if available_features:\n",
    "            f.write(f\"\\n# {category} Features ({len(available_features)})\\n\")\n",
    "            for feature in available_features:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(f\"Feature list export complete: {len(modeling_features)} modeling features identified\")\n",
    "\n",
    "print(f\"\\nFEATURE ENGINEERING COMPLETE!\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Final dataset: {final_dataset.shape[0]:,} observations × {final_dataset.shape[1]} features\")\n",
    "print(f\"Exported to: {output_dir}\")\n",
    "print(f\"   • modeling_dataset.parquet (production-ready)\")\n",
    "print(f\"   • modeling_dataset_sample.csv (validation)\")\n",
    "print(f\"   • dataset_metadata.json (comprehensive info)\")\n",
    "print(f\"   • feature_list.txt (model preparation)\")\n",
    "\n",
    "print(f\"\\nREADY FOR PREDICTIVE MODELING!\")\n",
    "print(f\"   Next step: Build machine learning models for violation hotspot prediction\")\n",
    "print(f\"   Prediction targets available: immediate, tactical, binary classification, risk scoring\")\n",
    "print(f\"   Features ready: temporal, spatial, CUNY, adaptation, enforcement intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: From Reactive Analysis to Predictive Intelligence\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "This notebook successfully transformed raw MTA data into a feature-rich, model-ready dataset. We've moved beyond simple retrospective analysis to engineer a foundation for a forward-looking, predictive system. Our features capture:\n",
    "\n",
    "#### Temporal Intelligence\n",
    "- Creating nuanced time blocks like morning/evening rush, school hours, and CUNY class change windows.\n",
    "- Engineering seasonal and academic cycle features (month, semester period).\n",
    "- Tracking the enforcement timeline with features like 'days_since_ace_implementation'.\n",
    "\n",
    "#### Spatial Intelligence \n",
    "- Integrating comprehensive GTFS data (stops, routes, shapes) for all five boroughs.\n",
    "- Identifying violation hotspot clusters using DBSCAN on a 50,000-record sample.\n",
    "- Calculating spatial density to measure violation concentration in a given area.\n",
    "\n",
    "#### CUNY-Specific Intelligence\n",
    "- Building proximity features based on Haversine distance to 7 key CUNY campuses.\n",
    "- Classifying routes as 'CUNY-serving' based on the concentration of violations within a 500m buffer.\n",
    "- Creating interaction features to capture patterns specific to student travel times.\n",
    "\n",
    "#### Enforcement Adaptation Intelligence\n",
    "- Quantifying enforcement predictability at each stop using Shannon entropy.\n",
    "- Measuring repeat offender concentration and vehicle diversity at the stop level.\n",
    "- Detecting route-level learning curves by analyzing the trend of monthly violations over time.\n",
    "\n",
    "#### Multiple Prediction Targets\n",
    "- Engineering 9 distinct target variables for different use cases.\n",
    "- Creating targets for immediate forecasting (`violation_count_next_hour`) and tactical planning (`violation_count_next_day`).\n",
    "- Developing composite targets for risk and priority scoring (`violation_risk_score`, `deployment_priority`).\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1.  **Adaptation Analysis**: This is the first known system to quantify violator adaptation by modeling enforcement predictability (entropy), repeat offender concentration, and learning curves over time.\n",
    "\n",
    "2.  **CUNY-Centric Modeling**: We've built purpose-specific features to analyze and ultimately protect crucial student transportation corridors, moving beyond a one-size-fits-all approach.\n",
    "\n",
    "3.  **Multi-Horizon Forecasting**: By creating targets for the next hour, next day, and overall risk, our dataset enables a flexible system that can inform both immediate operational deployment and long-term strategic planning.\n",
    "\n",
    "### Impact on NYC Transit\n",
    "\n",
    "This feature-rich dataset is the engine for a system that enables:\n",
    "- Proactive resource deployment using our 24-hour hotspot forecasts.\n",
    "- Data-driven protection for key student transit corridors.\n",
    "- Smarter enforcement that can adapt to predictable, systematic violator behavior.\n",
    "- Optimized resource allocation based on data-driven priority scores.\n",
    "\n",
    "### Next Phase: Predictive Modeling\n",
    "\n",
    "The final exported Parquet file, `modeling_dataset.parquet`, is now ready for machine learning. The next step is to build, train, and evaluate models to:\n",
    "- Classify high-risk hotspots using our binary severity flags.\n",
    "- Forecast exact violation counts using regression models like LightGBM or XGBoost.\n",
    "- Generate the final, prescriptive deployment recommendations for the MTA.\n",
    "\n",
    "**Result**: Transforming the 97.5% ACE failure rate into a predictive system that deploys enforcement resources exactly where and when they'll be most effective, with special protection for CUNY student routes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
